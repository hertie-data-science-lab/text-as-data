---
title: "Text as data: Week 2"
author: "Will Lowe"
date: "`r Sys.Date()`"
output: 
  html_document:
    fig_width: 7
    fig_height: 7 
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(collapse = TRUE, comment = NA)
library(tidyverse)
library(rvest)
library(quanteda)
```

## Preprocessing

Three depths of pre-processing

1. Dross removal: syntactic noise, typos, html, etc.
2. Tidying up: tokenizing, multiword phrase joining, unit of analysis arranging 
3. Modeling: ...

These get steadily more substantive as they go down. We've experimented a bit 
with 2. Let's look at that a bit further and then consider 1, before 
launching into 3 next week.

### Level 2: Tidying up

Tokenization is a basic task and easy-*ish* in Western languages. 

- Accents and capitalization can be tricky

And downright difficult in Chinese, Japanese, and some other languages 
that do not use space the same way 

- These languages tend to lack complicated inflection and 
morphology, so there's that.

### Interlude on Chinese (and Japanese)

Spaces do not separate words. So one way to split it into tokens  (implemented 
in `quanteda` via `stringi` is to use a dictionary based tokenizer.

```{r}
zh <- "二级飓风“莎莉”在美国东部时间16日登陆美国墨西哥湾沿岸阿拉巴马州和佛罗里达州，尽管较后飓风减弱为热带风暴，但仍带来破坏性大风和持续强降雨，酿成严重水患，许多地区街道与民宅泡在水中、路树倾倒，超过55万户人家停电。"
# From https://www.zaobao.com.sg/realtime/world/story20200917-1085730

sally_zh <- tokens(zh)[[1]]
head(sally_zh) # first and only element
```

Match everything, and keep the longest words that make the whole text come out 
right (no dangling characters or non-words)

 - This is what the computer scientists call a 'napsack problem'
 
We can do better if we know subject specific vocabulary or use a good statistical 
model, particularly when for 'out of vocabulary words'


### Interlude on Arabic, Farsi, Hebrew etc.

These are written (mostly) left to right 

- just don't get confused

```{r}
ar <- "إعصار سالي يضرب الولايات المتحدة"
sally_ar <- tokens(ar)[[1]]
sally_ar[1] # really this is the first word
```

### Tokenizing 'harder'

In Western languages we often like to strip words down to their 
'stems'. In full linguistic generality this is hard.

- In lazy 'good enough for TADA' work, it's fairly easy

```{r}
en <- "Part of a bridge collapsed in Pensacola as 30 inches of 
 rain and storm surge turned streets into white-capped rivers Wednesday 
 after Hurricane Sally lurched ashore the Gulf Coast."
# from https://eu.usatoday.com/story/news/nation/2020/09/16/hurricane-sally-tracker-updates-landfall-alabama-florida-rain/5814379002/

sally_en <- tokens(en)
tokens_wordstem(sally_en)
```
Remember that `quanteda` can't read, so `bridg` is only bad if it aliases 
things that should not be aliased, not if it uniformly mangles things that 
should be the same in the same way!

### Selective replacement

One *sometimes* useful thing to do is make your own substantive equivalences at 
the token level, e.g.

```{r}
d <- dictionary(list(SALLY = c("Hurricane Sally", "Sally"),
                     WEATHER = c("rain", "storm", "hail", "winds")))
sally_en <- tokens_lookup(sally_en, d, exclusive = FALSE)
sally_en[[1]]
```
Don't get carried away here...

### Dross removal

Web scraping 101

```{r}
TrumpSOTU <- "https://www.presidency.ucsb.edu/documents/address-before-joint-session-the-congress-the-state-the-union-27"
# download.file(TrumpSOTU, destfile = "TrumpSOTU.txt")

trump_html <- read_html("TrumpSOTU.txt")
trump_cont <- html_node(trump_html, "div[class='field-docs-content']")
trump_txt <- html_text(trump_cont)
```

## Dross removal

How to get rid of `"Audience members. U.S.A.! U.S.A.! U.S.A.!"`?

Maybe we'd like to remove every line beginning "Audience members"

Time for regular expressions...

```{r}
str_extract_all(trump_txt, 
                regex("^Audience members.*$", multiline = TRUE))
```

```{r}
trump_txt <- str_replace_all(trump_txt,
                regex("^Audience members.*$", multiline = TRUE), "")
```
You can confirm yourselves that they're no longer there.

Time to get rid of the applause too. How would you do that?

