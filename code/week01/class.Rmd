---
title: "Text as data: Week 1"
author: "Will Lowe"
date: "`r Sys.Date()`"
output: 
  html_document:
    fig_width: 7
    fig_height: 7 
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(collapse = TRUE, comment = NA)
library(tidyverse)
```

## Constructing corpora from plain text files

We'll start in the ideal situation: you have a folder of nicely titled text 
files (in a single text encoding, ideally UTF-8) and you would like to make 
them into a corpus for processing.

One such folder is `texts/uk-election-manifestos`, a 
a mostly comprehensive collection of party platforms for the main 
UK political parties in the post war period. Plus [ahem] some others, c.f. 
the `MRLP`.

The most useful tool here is `readtext` from the package of the same name:
```{r}
library(quanteda)

library(dplyr) # for easier data frame manipulation 
library(readtext) # for getting document and their info into a data frame 

COURSE <- "~iqmr" # where we keep all the course data
```

```{r}
manif_path <- "texts/uk-election-manifestos/"
manifestos <- readtext(manif_path)
head(manifestos)
```

There's a lot of useful information about these manifesto files 
that is encoded in the file title, so it's worth grabbing it as we read 
everything in.  Let's try it again:

```{r}
docinfo <- c("country", "national", "year", "language", "party")
manifestos <- readtext(manif_path,
                       docvarsfrom = "filenames",
                       docvarnames = docinfo)
head(manifestos)
```

Here we've specified that there are non-text fields (using some heuristics about what 
counts as a separator, which happily include the our `-`) encoded in the title 
and provided our own names for them. Had we not provided names they would have 
arrived in columns labeled `docvar1` through `docvar5`. 

`manifestos` is a data frame, so you can do anything to it that you'd normally 
do with a data frame, e.g. add or remove columns, merge in information from 
another source, and extract subsets.  Here we will do the bare minimum: 
remove the variable fields that are uninformative. `country` is always 
"UK", `national` is always "natl", and the language is always "en" (English).

```{r}
# easiest to use dplyr here
manifestos <- select(manifestos, 
                     doc_id, text, year, party) 
manifestos
```

The next step is to get this into a `corpus`.
```{r}
manif_corp <- corpus(manifestos) 
manif_corp
```

### A data frame from another source

This was straightforward because `{readtext}` (the package called "readtext") 
is designed to work with the `corpus` function. But often we have a data 
frame from some other source. In that case 
we need to tell the function which column is a unique document identifier 
(readtext made one from the filename called `doc_id` which is what `corpus` 
will look for unless we say otherwise.) We also need to say which column 
holds the text, e.g. like this:
```{r, eval = FALSE}
my_corp <- corpus(my_data_frame, 
                  docid_field = "my_id", text_field = "my_text")
```
(don't run this, it's just an example). 

If `my_dat_frame` has text in several columns, you'll need to bundle 
them together, e.g. like this:
```{r eval = FALSE}
# join title and body with two newlines (a 'paragraph')
my_data_frame <- mutate(my_data_frame, 
                        text = paste(title, body, sep = "\n\n"))
```

## Corpus structure

A corpus has two important components, the texts themselves, and information 
about them called 'docvars'. These are also the names of the functions used 
to extract them:
```{r}
# the top few docvars
head(docvars(manif_corp))
```
This is an ordinary R data frame. To get vectors of individual docvars, add 
the column name, e.g. here are the party names of the last few parties listed 
in the corpus
```{r}
tail(docvars(manif_corp, "party"))
```

When extracting texts, the same caveats 
as for `readtext` apply. `texts` returns a character vector, each of whose 
elements is a complete document, so printing all of them to screen can 
be... tiring.
```{r}
txts <- texts(manif_corp)
# show the first 100 characters of the first text
substr(txts[1], 1, 100)
```

Now that things are wrapped up in a corpus we can use the corpus functions. 

We can start by getting a bit more information using `summary`
```{r}
# summary restricted to the first 10 documents
summary(manif_corp, n = 10)
```
Note that, by default - that is without specifying `n` above, `summary` will 
report just the first 100 document. You can make this longer as well as shorter by setting 
`n` as above.  (Sometimes its useful to know that the output of `summary` is also a data 
frame, in case you want to keep the contents for later.)

The information in summary is available in separate functions if you need them, 
```{r, eval = FALSE}
ndoc(manif_corp) # document count

docnames(manif_corp) # unique document identifiers
ntype(manif_corp) # types in each document
ntoken(manif_corp) # tokens in each document
nsentence(manif_corp) # sentences in each documenta
```

Most often we will want to subset on the basis of the docvars, e.g. 
here we make another corpus containing just the three main parties in 
elections since 2000. 

```{r}
main_parties <- c("Lab", "Con", "LD")
manif_subcorp <- corpus_subset(manif_corp, 
                               year > 2000 & party %in% main_parties)
summary(manif_subcorp)
```

Two other useful corpus functions are `corpus_sample` for when your corpus 
is very large and you want to work with a random smaller set of 
documents, and `corpus_trim` which can remove sentences (or paragraphs or documents if you prefer) that have fewer or more than than a specified 
number of tokens in them. Note that if there are no sentences left after removing those deemed too 
short, then the document is removed from the corpus.  Usually this 
is what you want.

If you are working with transcripts, e.g. debates, then it is often useful 
to be able to specify a regular expression for the speaker marker. Here's a 
real, if complex example from the data preparation process of a debate we 
will examine later in teh course 
```{r, eval = FALSE}
regex("((^The )|(^Mrs. )|(^Mr. )|(^Ms. ))([eA-Z ]{2,}). ")
```
This matches lines that begin with an honorific or "The", continue 
in a namelike fashion, then end in a period, followed by a space. It 
captures the first part of lines like
```
The Speaker. All rise...
```
and 
```
Mr Hyde. I strenuously object...
```


## Looking into Bara et al.

The original debate: ["Medical Termination of Pregnancy Bill"](https://api.parliament.uk/historic-hansard/commons/1966/jul/22/medical-termination-of-pregnancy-bill) originally studied by Bara et al.

Let's start by reading in the data from the abortion debate analyzed by Bara et al.  
I've concatenated each speaker's contributions into a single file.

This is certainly not the only way to think about analyzing this data, 
but it's what Bara et al. did. The other ways are also in the data 
folder then read in some text files and make a `corpus` from them
```{r}
load("data/corpus_bara_speaker.rda")
summary(corpus_bara_speaker)
```
Those word and sentence counts come from the `ntype`, (vocabulary size)
`ntoken` (word count), and `nsentence` (sentence count)
functions.

Here are all the speakers that voted 'no' at the end of the debate and said more than 100 words during it
```{r}
no_corp <- corpus_subset(corpus_bara_speaker, 
                         vote == "no" & ntoken(corpus_bara_speaker) > 100)
no_corp
```
(Goodbye Mr Mahon)

Finally, it's sometimes convenient to be able to switch between thinking 
sets of documents to sets of paragraphs, or even
sentences.
```{r}
para_corp <- corpus_reshape(corpus_bara_speaker, 
                            to = "paragraphs") # or "sentences"
head(summary(para_corp)) # Just the top few lines
```
Happily we can always reverse this process by changing `to` back to 
"documents".

Let's explore a little more by looking for the key terms in play.  
One way to do this is to look for collocations.  The collocation finder functions operates on the *tokens* of the corpus, so we extract them first
```{r}
toks <- tokens(corpus_bara_speaker)
```
Structurally, a tokens object is a `list` of `character` vectors, so to get the 
tokens for the 10th document 
```
toks[[10]]
```
and to get a shorter list containing only first through third documents' tokens
```
toks[1:3]
```
There are a lot of useful dedicated tokens-processing functions in `{quanteda}`
all beginning with `tokens_`:
```
chunk compound keep ngrams remove replace sample 
segment select split subset subset tolower toupper
wordstem lookup
```
tokens objects carry their docvars around with them too, so you can use 
`tokens_subset` to get the tokens for e.g. just the speakers who voted "yes".

But let's cut straight to the collocation finding tools
```{r}
colls <- textstat_collocations(toks)
head(colls, 20)
```
This is disappointing unsubstantive, but if we work a bit harder we can get better
results.  First we'll remove those stopwords (leaving gaps where they were).
```{r}
toks2 <- tokens_remove(toks, stopwords(), padding = TRUE)

toks2[[1]][1:20] # first 20 tokens of document 1
```
Now rerun the function, maintaining the capitalization
```{r}
coll2 <- textstat_collocations(toks2, tolower = FALSE, size = 2)
head(coll2, 20)
```
We can also ask for three word collocations
```{r}
coll3 <- textstat_collocations(toks2, tolower = FALSE, size = 3)
head(coll3, 30)
```
If we're *really* serious about collocation hunting, it's 
probably best to use a dedicated package,
e.g. phrase machine (`devtools::install_github("slanglab/phrasemachine/R/phrasemachine")` but you'll need java installed and R connected to it first, I'll try to get this going in the cloud)


### Keywords in context

Since this is an abortion debate, let's see how the honorable folk talk about 
mothers and babies. We'll use the 'keyword in context' function `kwic`
```{r}
kw_mother <- kwic(corpus_bara_speaker, "mother*", window = 10)
head(kw_mother)
```
KWICs can get quite large, but if you want to see it all
```{r, eval = FALSE}
View(kw_mother)
```
will open a browser with the whole thing.

There is much less talk of babies than of mothers.
In this debate, the other major actors are doctors and their 
professional association, and a small amount of religious content.
We can investigate the same way.

If you want to look at phrases, e.g. "medical profession" or "human life"
in context there are two approaches.
Either we can use `tokens_compound` to force them into a single token and
look for that (fiddly) or we can use `phrase` inside the `kwic` 
function.
```{r}
kwic(toks, phrase("medical profession"))
```

For reference, here's how to take the first route
```{r, eval = FALSE}
toks <- tokens(corpus_bara_speaker)
phrases <- phrase("medical profession", "human life"))
toks <- tokens_compound(toks, phrases)
```
and then
```{r, eval = FALSE}
kwic(toks, "medical_profession")
```

The output of `kwic` is simply a data frame, so one thing that's often useful 
is to treat the left and right sides of the kwic as a document (about babies)

Although `{quanteda}` can construct corpora directly from kwic
objects itself, it separates left and right contexts, which is usually 
not what we want. Fortunately it's straightforward to 
paste together the 'pre' and 'post' keyword contexts and do it ourselves:
```{r}
babes <- kwic(corpus_bara_speaker, "babi*", window = 10)

babes_df <- tibble(speaker = babes$docname, 
                   text = paste(babes$pre, babes$post, sep = " "))
corp_babes <- corpus(babes_df)
summary(corp_babes)
```
This is just the kind of corpus that we might apply a content 
analysis dictionary to if we wanted topic statistics 
*conditional on talking about babies*.

Here we'll look at collocations in context of babies
```{r}
textstat_collocations(corp_babes, size = 2)
```

## Constructing a document feature matrix

Returning to the full corpus, we will often want to 
construct a document term matrix. `{quanteda}` calls this 
a 'dfm' (document feature matrix) to allow that we will often 
count things other than words.
```{r}
corpdfm <- dfm(corpus_bara_speaker) # lowercases by default, but not much more
dim(corpdfm)
featnames(corpdfm)[1:40] # really just colnames
docnames(corpdfm)
```
But let's remove some things that aren't (currently) of interest to us
```{r}
corpdfm <- dfm(corpus_bara_speaker, 
               remove = stopwords(), 
               remove_punct = TRUE,
               remove_numbers = TRUE)
dim(corpdfm) # a bit smaller
featnames(corpdfm)[1:40]
```
We *could* also stem
```{r}
stemdfm <- dfm(corpus_bara_speaker, 
               remove = stopwords(), 
               remove_punct = TRUE,
               remove_numbers = TRUE, 
               stem = TRUE)
dim(stemdfm) # about 1000 fewer 'word's
featnames(stemdfm)[1:40]
```
Later we'll apply a dictionary, but since *its* entries 
aren't stemmed we'd confuse it by stemming the source material.

### Making use of the docvars

One very convenient feature of the `dfm`, `tokens`, and `corpus` is that that they keep 
our docvars squirreled away. so we can subset 
in the same way as we did with the corpus object
```{r}
dfm_subset(corpdfm, vote != "abs") # remove abstentions
```

For example, if we really 
want a dfm with three rows made from collapses the word counts for 
the no voters, the yes voters, and the abstainers, we can 
specify this when we make the dfm
```{r}
corpdfm_votes <- dfm(corpus_bara_speaker, 
               remove = stopwords(), 
               remove_punct = TRUE,
               remove_numbers = TRUE,
               groups = "vote")
dim(corpdfm_votes) 
docnames(corpdfm_votes)
```

Alternatively, we can collapse down to groups later
```{r}
# From above, corpdfm is organized by speaker
corpdfm_votes <- dfm_group(corpdfm, "vote")
corpdfm_votes
```

For modeling, we'll often want to remove the low frequency 
and idiosyncratic words
```{r}
smallcorpdfm <- dfm_trim(corpdfm, min_termfreq = 5, min_docfreq = 5)
dim(smallcorpdfm)
```
where `min_count` removes any word that occurs less than 5 times and `min_docfreq` removes any words that occurs any number of times but in 
fewer than 5 different documents. That makes things a lot smaller. 

## Comparing frequencies

If we are interested in comparing the usage of groups of speakers, 
we can use the `textstat_frequency` function. 
There are a lot of `textstat_` functions, e.g. 
```
select dist simil entropy frequency keyness lexdiv readability
```

Here we'll examine what 
sorts of words eventual yes and no voters used
```{r}
corpdfm_yesno <- dfm_subset(corpdfm, vote != "abs")

textstat_frequency(corpdfm_yesno, 
                   n = 20, groups = "vote")
```
as is often the case, raw counts are not so informative, so 
we can instead ask for terms which differ statistically across 
yes and no voters in a more statistical fashion
```{r}
dfm_yesno <- dfm_group(corpdfm_yesno, "vote")

no_terms <- textstat_keyness(dfm_yesno, "no")
head(no_terms, 25)
yes_terms <-  textstat_keyness(dfm_yesno, "yes")
head(yes_terms, 25)
```
where we had to collapse the dfm over vote first, so we 
could specify that the "yes" ("no") voters were the 'target' 
category.

If we had been interested in the personal linguistic 
style of one of our speakers, we would not have had to 
group the dfm. For example, here are terms preferentially 
used by Mr. Norman St John-Stevas
```{r}
nsjs_terms <- textstat_keyness(corpdfm, "Mr Norman St John-Stevas")
head(nsjs_terms, 10)
```

We can also use `corpdfm_yesno` to see to what extent the 
Speaker's promise to give equal time to both sides of the debate
was fulfilled.

It's hard to know whether the debate was persuasive since we do not know the speakers prior beliefs (though we could find out from their previous debates) so let us assume that there was no substantial persuasion and we'll assume that no speaker spoke particularly slowly.  
These imply that we can proxy speaking time with number of words said, 
and side with final vote.

```{r}
ntoken(dfm_yesno) # words on each side
table(docvars(corpdfm, "vote")) # speakers on each side
```
Although three quarters of the speakers voted "yes", it seems that floor time was about two to one "yes" to "no" voters.



<!-- load(url("http://varianceexplained.org/files/trump_tweets_df.rda")) -->
<!-- trump_tweets_df %>% extract(statusSource, into = "phone", regex = "Twitter for ([a-zA-Z]+)") %>% select(text, phone, created) %>% corpus %>% corpus_subset(phone != "iPad") %>% tokens(remove_url = TRUE, remove_twitter = TRUE, remove_punct = TRUE, remove_numbers = TRUE) %>% dfm(group = "phone", remove = stopwords()) %>% dfm_subset(fff, ntoken(fff) > 0) -> fff -->
<!-- ff %>% textstat_keyness(2) %>% head(20) -->
<!-- ff %>% stm -->
