---
title: "Assignment"
author: "Week 9"
date: "12/11/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = "")
```

In this assignment we'll consider the scaling analysis from the 
slides: Comparing German and British parties and topics

For convenience, computational and otherwise, we'll use the `ca` 
package to do our scaling. For reference, this is the least squares
version of the model shown in the slides and consequently tends 
to give very similar results but rather quicker.

Our data will be the results of a long running large scale 
human text analysis project once called the Comparative 
Manifestos Project (CMP) and subsequently various other things
that those of us who work in this area have forgotten. So we'll 
call it the CMP data. More about it can be found over here at the 
WZB: https://manifesto-project.wzb.eu

In short, lots of coders identify policy assertions in 
the platforms / manifestos / statements of policy preference 
of parties across Europe, North and South American, and a few other 
places, and code them into one of about 56 categories
(this changes over time but there are always a core 56). It's a 
histprical record and generates count data of the kind that a 
topic model or sentence classifier might automate, e.g. in 2002 the 
German Free Democrat Party had 14 sentences assigned to policy category `101` (Foreign Special Relationships: Positive) 
out of 1982 that were codable.

The gory details of the categories, parties, etc. can be found 
in the pdf codebook in the `data` folder which you can peruse 
at your leisure, but need not detain us now.

This kind of count data is also exactly the same sort of data as would come out of dictionary based content analysis, or if we had counted words rather than categories or topics. Consequently it can be scaled just the same 
way as we scaled the debate to infer speaker/document positions in class.

We'll start by loading the data set, correcting a small coding 
mistake, and turning the CMP's percentages back into the counts 
they were originally so we can treat it like the document 
feature matrix that it fundamentally is.

But before we begin, a quick note: you should feel free to get help if you 
are having trouble figuring out what the code is doing or how to adjust it.
Lightly adjusting the existing code is quite sufficient for this assignment, 
although impressive own coding is much appreciated.

## The data

```{r, message=FALSE, warning=FALSE}
library(tidyverse)
library(ggrepel) 

theme_set(theme_minimal())
```

```{r,message=FALSE}
# Read in, ignoring the parse failures because they're on a
# variable we won't use
allcmp20 <- read_csv("data/MPDataset_MPDS2020a.csv")
```

Now to give a nice abbreviation to the Pirates since the data doesn't have one 
```{r}
allcmp20$partyabbrev[allcmp20$partyname == "Pirates"] <- "Avast!"
```
Now we'll re-inflate the counts and throw away all the electoral 
and other info in the data that we won't use
```{r}
cmp20 <- allcmp20 %>%
  select(edate, countryname, partyname, partyabbrev, 
         total, voteper = pervote, uncoded = peruncod,
         matches("per\\d\\d\\d$")) %>% # per%d%d%d% are subcategories, so we ignore them
  mutate(edate = as.Date(edate, format = "%d/%m/%Y"),
         eyear = lubridate::year(edate), # make a nice year just in case we want to filter with it
         label = paste(partyabbrev, eyear, sep = ":"), # for graphing
         across(starts_with("per"), function(x) round(total * (x/100)))) 
```
Phew. That was a fairly typical bit of data cleaning code, which 
takes up a lot of data science-ing time. Study it a little 
if you think this is in your future. But we'll just use it 
below.

## The topic metadata

Now to pull in the labels corresponding to the meaning of each topic. This is metadata, since it is a mapping from the obscure
column header codes to two facts about each code.
```{r,message=FALSE}
itemcodes <- read_csv("data/itemcodes.csv")
head(itemcodes)
```
Here we've got a table of basic information about the CMP's coding 
scheme and also a "rile.valence" that are supposed to be right (marked here as 1) or left (marked here as -1), or uncategorized (0)
We'll use them later.

Now for the counts

## Reconstructing counts 

The column `per101` is now the *count* of sentences coded into category 101. Let's extract the document feature matrix part so we can 
give it to scaling models
```{r}
counts <- select(cmp20, starts_with("per"))
counts[1:3, 1:7] # quick peek at the top left corner
```
and switch into a old-style R representation that has
rownames and colnames like you remember before you knew anything 
about the tidyverse.
```{r}
mat <- data.matrix(counts)
rownames(mat) <- cmp20$label
colnames(mat) <- itemcodes$name
```

## Analysis: Germany 

This is historical data, but we'll start our analysis somewhat 
arbitrarily in 1990, just after the fall of the Wall.
```{r}
oldest <- as.Date("1990-01-01") 
```
and focus on Germany
```{r}
de_mat <- mat[cmp20$edate > oldest & cmp20$countryname == "Germany",]
de_mat[1:3, 1:4]
```
Looks like dfm, right? If we've got this kind of object available then 
we're ready to scale.

But before we do that, let's grab the Germany part of `cmp20` since it's got all kinds of useful things in it
```{r}
de_rest <- filter(select(cmp20, -starts_with("per")), 
                  edate > oldest, 
                  cmp20$countryname == "Germany")
```

## Model 

Now to scale these counts and see what they tell us about parties.
We'll use `ca` rather than `wordfish` because we'd like to 
be able to efficiently scale in multiple dimensions. So let's load 
the package and run the scaling function on `de_mat`. Call that model `mod1`
```{r}
library(ca)

mod1 <- ca(de_mat)
```
`mod1` has a lot of stuff in it. Of primary interest to use
are the document positions "theta" and the word positions "beta".
CA calls these row coordinates and column coordinates respectively
(which makes sense) and these are tucked inside the model
in matrix form. 

Extract them and call them `theta` and `beta` respectively
(Remember that we're interested in *columns* of the row and column coordinate matrics, as you can tell by examining their sizes)
```{r}
theta <- mod1$rowcoord[,1] # thetas
beta <- mod1$colcoord[,1] # betas
```
Now take a quick look at the third and fourth elements of theta. Note that the SPD is considered to be a left wing party relative to the FDP.

So it looks like we have one of those arbitrary 
sign 'problems'. We'll fix it by enforcing that the SPD is to the left of i.e. less than the FDP and flipping the signs of all our parameters 
to make it so if we need to
```{r}
switcheroo <- ifelse(theta[3] < theta[4], 1, -1)
```
Now you *multiply* both `theta` and `beta` by `switcheroo` and get something nicely interpretable. Do that to both!
```{r}
theta <- switcheroo * theta
beta <- switcheroo * beta
```

Next up, plot these party positions as they vary over elections. We don't have uncertainty estimates (yet) so don't worry about those. 

You may need to do a bit of data frame construction before plotting;
you can get election date `edate`, and country names from `de_rest`.
```{r}
dat <- data.frame(position = theta, 
                  country = de_rest$countryname,
                  party = de_rest$partyabbrev,
                  election = de_rest$edate)

ggplot(dat, aes(election, position, colour = party)) + 
  geom_point() + 
  geom_line() +
  facet_grid(rows = vars(country)) 
```

## Robustness to topic choices

How different does this plot look if you only use the RILE (+1 and -1) topics in the model? (Call this model `mod2` so we don't get confused)

```{r}
rile_de_mat <- de_mat[, itemcodes$rile.valence != 0]
mod2 <- ca(rile_de_mat)
theta <- mod2$rowcoord[,1]
switcheroo <- ifelse(theta[3] < theta[4], 1, -1)
dat <- data.frame(position = theta, 
                  country = de_rest$countryname,
                  party = de_rest$partyabbrev,
                  election = de_rest$edate)

ggplot(dat, aes(election, position, colour = party)) + 
  geom_point() + 
  geom_line() +
  facet_grid(rows = vars(country)) 

```
In particular, would our opinions about the AfD and Die Linke change depending on the topics we used, and if so, how?
```

```

## Uncertainty with CA

Its time to get some confidence intervals on things. These will be 
useful if we want to know, e.g. the probability that one party is 
to the left of another, when their estimated positions are quite close.

For this we'll need joint sampling distribution for theta (and beta).
From that we can compute - well, anything really. since we shouldn't 
be particularly omfortable making Normality assumptions and we don't
want to attempt any tricky mathematics (really, we don't), then 
we'll use the bootstrap as a way to sample parameter estimates.

For those who have not met a bootstrap, the idea is that we resample 
the existing data set to create 'bootstrap' data sets that represent
counts we could have seen but did not. There are a lot of subtle ways to 
do this, but we'll take a blunt approach (that tends to agree with human uncertainty levels in some cases that have been examined in detail).

First we'll normalize each row of `counts` to sum to 1 and then 
assert that these are topic generation probabilities. Then we'll 
take a sample of the same size as the original row total, using these
probabilities. That will generate a new 'document' of the same length
as the original, but slightly different counts. Once we've
done it for each line of `counts` we'll take those counts, fit a model
to it, and extract its estimated `theta` and `beta` and keep them for later. In the end we'll have B vectors of `theta`s and B vectors of `beta`s, which we can summarise to get a standard error, or use in 
other comparisons. 

Here's some hopefully straightforward code to get you going

```{r}
# grab the estimates from the real data
origmod <- ca(de_mat)
theta <- origmod$rowcoord[,1]
beta <- origmod$colcoord[,1]
signflip <- ifelse(theta[3] > theta[4], -1, 1)
theta <- signflip*theta
beta <- signflip*beta

# how many bootstrap samples to use
B <- 500

# some constants to save typing
N <- nrow(de_mat)
V <- ncol(de_mat)

# how long each document is
doclens <- rowSums(de_mat)

# take the original counts and divide each element by 
# it's row total
probs <- sweep(de_mat, 1, doclens, FUN = '/')

# make a place for each bootstrap data set to land in
boot_sample <- matrix(0, nrow = nrow(de_mat), 
                      ncol = ncol(de_mat))
# this will hold all our beta estimates
betas <- matrix(0, nrow = V, ncol = B)
rownames(betas) <- itemcodes$name
# this will hold all our theta estimates
thetas <- matrix(0, nrow = N, ncol = B)
rownames(thetas) <- de_rest$label

# a function to create a new data set, fit a model, flip 
# signs as necessary, extract theta and beta, and store them
boot_mat <- function(b) {
  # report every 100 iterations
  if (b %% 100 == 0) 
    cat(".") 
  # resample a new data set
  for (i in 1:N)
    boot_sample[i,] <- rmultinom(1, prob = probs[i,], 
                                 size = doclens[i])
  # fit model to this data set
  res <- ca(boot_sample)
  # extract the position estimates
  theta <- res$rowcoord[,1]
  beta <- res$colcoord[,1]
  # if SPD > FDP, make a sign-flipping multiplier to identify
  signflip <- ifelse(theta[3] > theta[4], -1, 1)

  # record these values in the boostrap sample collection 
  # that lives outside the loop
  thetas[,b] <<- signflip * theta
  betas[,b] <<- signflip * beta
} 

# run this to get B bootstrap estimates in thetas and betas
for (b in 1:B) boot_mat(b) 
```

How to use these things? Well, you could use this code to get making 
95% confidence intervals for theta:
```{r}
# summarise the estimates in each row
quants <- apply(thetas, 1, quantile, 
                probs = c(0.025, 0.975))
quants <- t(quants) # flip it over
names(quants) <- c("lwr", "upr") # and rename
head(quants)
```
OK, now your turn. Remake that first graph but with uncertainty
estimates. Add them however you think makes sense visually.
```{r}
plottable_thetas <- data.frame(country = de_rest$countryname,
                               party = de_rest$partyabbrev,
                               date = de_rest$edate,
                               theta = theta, 
                               quants)
names(plottable_thetas) <- c("country", "party", "date", "theta", 
                             "lwr", "upr")

ggplot(filter(plottable_thetas, country == "Germany"),
       aes(date, theta, color = party, fill = party, 
           ymin = lwr, ymax = upr)) + 
  geom_line() + 
  geom_ribbon(colour = NA, alpha = 0.4)
  geom_ribbon
```

It seems like we aren't quite sure about the AfD's position in 2013. To get a sense of that uncertainty, make a density plot (or a ggridges plot if you like those more) showing each party's position estimates in 2013; use alpha transparency so we can see the overlaps.

You may find it useful to have things in long format 
```{r}
elec2013 <- thetas[de_rest$eyear == 2013, ]
party2013 <- de_rest$partyabbrev[de_rest$eyear == 2013]
samps2013 <- data.frame(elec2013, party = party2013)
longsamps2013 <- pivot_longer(samps2013, -party)
head(longsamps2013)
```

```{r}
ggplot(longsamps2013, aes(value, fill = party)) + 
  geom_density(alpha = 0.5, color = NA)
```

## Inference

Being just a little fast a loose, statistically speaking, how might we estimate the probability that the AfD was in fact to the *left* of the CDU in this election?
```{r}
mean(elec2013[5,] > elec2013[7,])
```

## Centers of gravity

One (admittedly crude) way to get a sense of where a country is 
in ideological terms is to weight the party positions by their 
vote shares. We have the vote shares as percentages in the `voteper`
variable, so let's you transform those into proportions and get an 
average position for Germany in each election. To be explicit, that's
$$
\sum^\text{Parties}_p = \text{vote proportion}_p \hat{\theta}_p
$$
for each year. What do you see?
```{r}
de_rest <- mutate(de_rest, 
                  voteprop = voteper/100,
                  position = theta)
cog <- summarize(group_by(de_rest, edate),
                 cog = sum(voteprop * position))

ggplot(cog, aes(edate, cog)) + 
  geom_point() +
  geom_line() + 
  ylim(-2, 2)
```




