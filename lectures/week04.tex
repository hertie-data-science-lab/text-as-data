\documentclass{hertieteaching}
\usepackage{bbm}

\title{Content Analysis Dictionaries 2}

\begin{document}

{\setbeamertemplate{footline}{}
\begin{frame}
\maketitle
\end{frame}}
\addtocounter{page}{-1}

% model structure choices
% preprocessing
% strategic considerations

%\begin{frame}{Recap}
%
%
%The \textit{message} of the document is
%$$
%[\theta_1, \ldots, \theta_K]
%$$
%where each element of the message is
%$$
%\theta_k ~=~ P(Z=k) ~=~ \frac{1}{N} \sum^N \mathbbm{1}(Z_i = k)
%$$
%
%Quite generally, the optimal \textit{estimator} for this is 
%\begin{align*}
%\hat{\theta}_k & ~=~ \frac{1}{N}\sum^N_i P(Z_i=k \mid \{ W \}) \\
%              & ~=~ \frac{1}{N}\sum^N_i P(Z_i=k \mid W_v[i]) & \text{(because we use a content analysis dictionary)}
%\end{align*}
%where we've replaced the unknown $Z_k$ with our best guess given the word type.
%
%
%\end{frame}
%\begin{frame}{Recap}
%Why is this the optimal estimator?
%
%Example (simple):
%\begin{itemize}
%  \item If topic $k$ is the \textit{only} one that generate words of type $W_{v[i]}$, that is if
%  $$
%  P(W_{v[i]} \mid Z = k) = 1
%  $$
%  then definitely $P(Z_k = k \mid W_{v[i]}) = 1$
%  \item So we should count 1 for topic $k$ 
%\end{itemize}
%
%\end{frame}
%\begin{frame}{Recap}
%
%Example (trickier): 
%\begin{itemize}
%  \item Topics $j$ and $k$ generates $W_{v[i]}$, e.g. 
%  \begin{align*}
%    P(W_{v[i]} \mid Z = k) &= 0.03 & 
%    P(W_{v[i]} \mid Z = j) &= 0.01
%  \end{align*}
%  but \textit{no other topics} do. 
%\end{itemize}
%then Rev. Bayes says
%\begin{align*}
%  P(Z=k \mid W_{v[i]}) & ~=~ P(W_{v[i]} \mid Z = k) P(Z=k) / P(W_{v[i]})\\
%\intertext{and if we don't have a reason before seeing any words  to think topic $k$ is more or less frequent than $j$}
%  & ~=~ \frac{0.03}{(0.03 + 0.01)} ~=~ 3/4
%\end{align*}
%Since only two topics generate this word the other topic gets the remaining $1/4$
%\end{frame}
%
%\begin{frame}{Recap}
%
%In short:
%\begin{itemize}
%  \item We \textit{redistributed} one count across the two topics that might of generated it, in proportion to how likely they were to have generated it.
%\end{itemize}
%We cannot do better than this! \pause We can do worse though.
%
%Best case scenario:
%\begin{itemize}
%  \item Our dictionary sees this word and assigns it to topic $k$ (because that's most likely) then our measurement error is\ldots 1/4
%\end{itemize}
%
%
%
%  
%  
%\end{frame}


%\begin{frame}{Recap}

%\begin{frame}{}
%  
%  with probability 0.1, topic $j$ generates it with probability 0.025, and no other topics generate it at all, then
%  \begin{align*}
%  P(W_{v[i]} \mid Z = k) = 3/4 & ~~\text{and}~~ P(W_{v[i]} \mid Z = j) = 1/4
%  \end{align*}
%\end{itemize}
%
%\end{frame}
%
%if there's a 50\% chance that any a $W_i$ was generated by the $k$-th topic, then we count it 50%
%
%
%\frac{1}{N}\sum^N_i \textbb{I}[Z_i=k]
%
%
%
%
%First count up all the `hits', where $Z=k$
%$$
%Z_k ~=~ \sum^N_{i} P(Z = k \mid W_i)
%$$
%then divide by the sum
%$$
%\hat{\theta}_k ~=~ \frac{Z_k}{\sum^K_{j} Z_j}
%$$
%and that's our estimate of the document content
%
%
%
%
%A simple example, elaborating on the measurement error we saw in the previous lecture
%
%Let's work with a tiny subset of the \textcite{Laver.Garry2000} dictionary. This time we'll use some words from the `neutral' category too
%
%\medskip
%\begin{center}
%\begin{tabular}{ll} \toprule
%  \textbf{state reg}     & \textbf{centrist} & \textbf{market econ} \\ \midrule
%          accommodation  & accountants & assets\\
%          age            & bargaining  & bid\\
%          ambulance      & electricity & choice*\\
%          \ldots         & \ldots 
%\end{tabular}
%\end{center}
%
%\end{frame}
%
%\begin{frame}{Recap}
%
%Target 
%
%
%\end{frame}
%
%\begin{frame}{Topics}
%
%\begin{center}
%\begin{tikzpicture}
%\node(W) at (1.5,0)  [var,label=below:$W$]{};
%\node(Z) at (3,0)  [lat,label=below:$Z$]{};
%\draw(Z) -- (W);
%\draw[gray](1,-0.75) rectangle (3.75,0.5){};
%\node[gray] at (4,-0.6){\small N};
%\node(theta) at (4,1)  [lat,label=right:$\theta$]{};
%\draw(theta) -- (Z);
%\node(beta) at (-0.25, 0) [var,label=below:$B$]{};
%\draw(beta) -- (W);
%\draw(-0.75,-0.75) rectangle (0.5,0.5)[gray];
%\node(labk) at (-0.95,-0.6)  []{{\color{gray}\small K}};
%
%\end{tikzpicture}
%\end{center}
%
%
%$B$ is a `dictionary' that explains how often each word should be generated in each of the $K$ topics
%\begin{itemize}
%  \item An entry in the dictionary $B$ is a vector of word generation probabilities $\beta_k$ 
%\end{itemize}
%This week we will assume we \textit{know} $B$
%
%\pause
%Strictly speaking we will assume that we know enough about its \textit{inverse} $W \longrightarrow Z$ to say
%\begin{itemize}
%  \item for each word $W$, what its $Z$ is.
%\end{itemize}
%
%\end{frame}



\begin{frame}{Solutions: Some theological approaches}

\pause

\centerline{\includegraphics[scale=0.5]{pictures/praying-skeleton}}

\centerline{``Thoughts and prayers''}

\end{frame}

\begin{frame}{Solutions: Do not sin in the first place}

\pause

An often non-obvious fact about content dictionaries:

\begin{itemize}
  \item \textit{Recall}: the proportion of words used that way that are in your dictionary
  \item \textit{Precision}: the proportion of words used the way your dictionary
  assumes they are used
\end{itemize}

\end{frame}

\begin{frame}{Sins}

Every field reinvents this distinction:

\begin{itemize}
\item
  precision and recall
  \item
  specificity and sensitivity
  \item
  users and producer's accuracy
  \item
  type 1 and type 2 error
  \item sins of omission and sins of commission
\end{itemize}

\end{frame}

\begin{frame}{Precision}

Keyword in context analyses (KWIC) allow you to scan all contexts of a word 

\begin{itemize}
\item
  How many of them \textit{are} the sense or usage you want?
\end{itemize}

Let's take a look at \texttt{benefit*} as a `pro government intervention in the economy' word

\end{frame}

\begin{frame}{}

\footnotesize
\begin{table}[ht]
\centering
\begin{tabular}{rlll}
  \hline
 & pre & keyword & post \\ 
  \hline
1 & also keep all the other & benefits & that pensioners currently receive , \\ 
  2 & regulation will have to have & benefits & exceeding costs , and regulations \\ 
  3 & and Controlled Immigration Britain has & benefited & from immigration . We all \\ 
  4 & positive contribution But if those & benefits & are to continue to flow \\ 
  5 & Northern Ireland brings & benefits & to all parts of our \\ 
  6 & their home , will also & benefit & first-time buyers . Empowering individuals \\ 
  7 & you help yourself ; you & benefit & and the country benefits . \\ 
  8 & you benefit and the country & benefits & . So now , I \\ 
  9 & result of our tax and & benefit & measures compared to 1997 . \\ 
  10 & result of personal tax and & benefit & measures introduced since 1997 , \\ 
  11 & , the savings on unemployment & benefits & will go towards investing more \\ 
  12 & trebled the number on incapacity & benefits & . We will help 17 \\ 
  13 & Work programme and reform Incapacity & Benefit & , with the main elements \\ 
  14 & main elements of the new & benefit & regime in place from 2008 \\ 
  15 & stronger penalties . To the & benefit & of business and household consumers \\ 
  16 & effective directive to provide real & benefits & to consumers and new opportunities \\ 
  17 & better.We are examining the potential & benefits & of a parallel Expressway on \\ 
  18 & ways to lock in the & benefit & of new capacity . We \\ 
  19 & are determined to spread the & benefits & of enterprise to every community \\ 
  20 & to get ahead , to & benefit & from improving public services\\
   \hline
\end{tabular}
\end{table}
\normalsize

\end{frame}

\begin{frame}{Precision}

Of the 20 instances, these are (arguably)
\begin{itemize}
  \item 6 used the way we expect from the topic
  \item 3 used in the opposite sense: anti-government intervention in the economy
  \item 11 used in ways that are neither
\end{itemize}

So\ldots 0.3 correct, 0.15 mistaken, and 0.55 unrelated `noise'
\begin{itemize}
  \item Perhaps not an amazing choice
\end{itemize}

There are two kinds of precision failures here with different consequences
\begin{itemize}
  \item Mistaking an \textit{topic-unrelated} word for this topic (11 of these)
  \item Mistaking a word used in the sense of a \textit{different} topic for this one (3 of these)
\end{itemize}

The first mistake does not really harm precision, but the second does

\end{frame}

\begin{frame}{Recall}

Bad recall is a mixture of two problems
\begin{itemize}
  \item Assigning words to a topic that are mostly used for a different one: $P(W \mid Z=k)<P(W \mid Z=j)$ but we assigned it to $k$ anyway
  \item Failing to assign a topic-informative word to any topic: Dictionary says $P(W \mid Z=k)=0$, but it's not. This is about \textit{coverage}
\end{itemize}

Let's consider coverage first

\end{frame}
\begin{frame}{Coverage}

One possible checking procedure:
\begin{itemize}
  \item Take a random matched sample of words not in the dictionary but present in the corpus e.g. match each dictionary word to another of the same frequency
  \item Examine their KWICs to see if they should have been assigned to a topic
\end{itemize}
If we were feeling even more energetic
\begin{itemize}
  \item Assign them their most likely topic manually
  \item Compare this $\tilde{\theta}$ to the dictionary's own estimate $\theta$
  \item These should not be wildly different
\end{itemize}
  
\end{frame}

\begin{frame}{Recall}

The other kind of mistake is difficult because the natural procedure is
\begin{itemize}
  \item Assign \textit{every word} (or at least every instance of a word that the dictionary knows about in a document) to a topic
  \item For each topic, see what proportion of times  the dictionary agrees it is in that topic
\end{itemize}
This also promises to be very tiring.

\end{frame}


\begin{frame}{Using precision to estimate recall}


However, two facts may help us:
\begin{itemize}
  \item We have a tireless computer available
  \item Recall and precision relate $P(W \mid Z)$ and $P(Z \mid W)$ respectively
  \item \ldots and we know how
\end{itemize}

For clarity, let's call the true topic of a word $Z$ as before, and the dictionary's idea of the topic of a word $\hat{Z}$ because it's kind of an estimate of that. So,
\begin{align*}
\text{Recall:} & ~~~~ \sum^K_k P(\hat{Z}=k \mid Z=k)\\
\text{Precision:} & ~~~~ \sum^K_k P(Z=k \mid \hat{Z}=k)
\end{align*}

\end{frame}

\begin{frame}{Using precision to estimate recall}

If we have a sense of dictionary topic precision, could we use it to get a sense of dictionary topic recall? Why yes, by borrowing methods from \textcite{King.Lowe2003}
\pause

According to the Rev. Bayes
\begin{align*}
P(\hat{Z}=k \mid Z=k) &~=~ \frac{P(Z=k \mid \hat{Z}=k) P(\hat{Z}=k)}{\sum^K_j P(Z=j \mid \hat{Z}=j) P(\hat{Z}=j)} \\
                      &~=~ P(Z=k \mid \hat{Z}=k) \frac{P(\hat{Z}=k)}{P(Z=k)}& \text{(recall is reweighted precision)}\\
                      &~\propto~ P(Z=k \mid \hat{Z}=k) P(\hat{Z}=k) 
\end{align*}
Conveniently
\begin{itemize}
  \item we don't need the denominator because it only ensure the recall measures add to one
  \item We can get $P(\hat{Z}=k)$ by running the dictionary over the entire corpus
\end{itemize}
  
\end{frame}

\begin{frame}{Using precision to estimate recall}



\end{frame}

%%%%%%%

\begin{frame}{Confession and forgiveness}
\protect\hypertarget{confession-and-forgiveness}{}

Under measurement error

\begin{itemize}
\tightlist
\item
  A observed category proportions are generated by a \emph{mixture} of
  categories
\item
  The weights for this mixture are the true category proportions
  \(P(Z=k) = \theta\)
\end{itemize}

\[
P(W) = \sum^K_k {P(W \mid Z=k)} P(Z=k)
\]

\end{frame}

\begin{frame}{Two applications of the mixture: 1. Naive Bayes}
\protect\hypertarget{two-applications-of-the-mixture-1.-naive-bayes}{}

\[
P(W) = \sum^K_k {P(W \mid Z=k)} P(Z=k)
\]

If we we ever observed Z we could learn a lot about \(P(W \mid Z=k)\).

\pause

At the word level we typically do not have access to \(Z\)s.

\end{frame}

\begin{frame}{Two applications of the mixture: 1. Naive Bayes}
\protect\hypertarget{two-applications-of-the-mixture-1.-naive-bayes-1}{}

But if we are willing to assume that all the words in a document have
the \emph{same} value of \(Z\), then we only need judgments about
document content to learn about \(P(W \mid Z=k)\).

And if \[
P(W \mid Z=k) = \prod P(W_j \mid Z=k)
\] then we are classifying documents using \emph{Naive Bayes} (Evans et
al. 2007)

\end{frame}

\begin{frame}{Two applications of the mixture: 2. Correction}
\protect\hypertarget{two-applications-of-the-mixture-2.-correction}{}

But back to words and their categories\ldots{}

\[
P(W) = \sum^K_k {P(W \mid Z=k)} P(Z=k)
\]

If we knew about the error process \(P(W \mid Z=k)\), we could
\emph{back out} the true proportions

\end{frame}

\begin{frame}{A general purpose linear approach}
\protect\hypertarget{a-general-purpose-linear-approach}{}

The category proportions are \[
P(W) = \sum^K_k {P(W \mid Z=k)} P(Z=k)
\] has the form \[
P = E \theta
\] where \(P\) are the coded proportions and \(E\) is the \(V \times K\)
coder error matrix, so \[
\theta = \text{E}^{-1} P
\] Typically we don't exactly know \text{E} so the result is an
approximation

\end{frame}

\begin{frame}{An application to human sentence coders}
\protect\hypertarget{an-application-to-human-sentence-coders}{}

Application to Mikhaylov et al.'s subjects coding New Zealand party
manifestos.

\begin{longtable}[]{@{}lllll@{}}
\toprule
& true & L & N & R\tabularnewline
\midrule
\endhead
code & L & 430 & 188 & 100\tabularnewline
& N & 254 & 712 & 193\tabularnewline
& R & 41 & 115 & 650\tabularnewline
\bottomrule
\end{longtable}

\end{frame}

\begin{frame}{Convert to error probabilities}
\protect\hypertarget{convert-to-error-probabilities}{}

Errors: \(E\)

\begin{table}[ht]
\centering
\begin{tabular}{rrrr}
  \toprule
 & L & N & R \\ 
  \midrule
L & 0.59 & 0.19 & 0.11 \\ 
  N & 0.35 & 0.70 & 0.20 \\ 
  R & 0.06 & 0.11 & 0.69 \\ 
   \bottomrule
\end{tabular}
\end{table}

\pause

Inverted: \(E^{-1}\)

\begin{table}[ht]
\centering
\begin{tabular}{rrrr}
  \toprule
 & L & N & R \\ 
  \midrule
L & 2.00 & -0.50 & -0.16 \\ 
  N & -1.00 & 1.75 & -0.37 \\ 
  R & 0.00 & -0.25 & 1.52 \\ 
   \bottomrule
\end{tabular}
\end{table}

\pause

\(\theta\) = 0.27, 0.38, 0.35

\pause

Observed P = 0.27, 0.43, 0.3

\end{frame}

\begin{frame}{Implications for derived measurements}
\protect\hypertarget{implications-for-derived-measurements}{}

If {[}L, N, R{]} were {[}20, 0, 10{]}

\begin{itemize}
\tightlist
\item
  true position: -0.33 on our previous left-right scale, ignoring N
\end{itemize}

Under measurement error we would \emph{expect} to see about {[}13, 9,
8{]}

\begin{itemize}
\tightlist
\item
  an attenuated -0.24 on our previous scale
\end{itemize}

Correcting this with the correct error matrix would recover the right
proportions, and so the right position

\end{frame}

\begin{frame}{Implementations and limitations}
\protect\hypertarget{implementations-and-limitations}{}

Hopkins and King (2010) and King and Lu (2008) implement this strategy

\pause

When coder errors are only \emph{estimated} the result hold in
expectation.

The tradeoff is between

\begin{itemize}
\tightlist
\item
  errors coders make
\item
  errors we make estimating the errors coders make\ldots{}
\end{itemize}

\end{frame}

\begin{frame}{Implementations and limitations}
\protect\hypertarget{implementations-and-limitations-1}{}

Linearity means we sometimes estimate proportions outside {[}0,1{]}

Alternatively we can assign distributions and work with the original
structure \[
{P(W)} = \sum^K_k {P(W \mid Z=k)} P(Z=k)
\] and that is exactly what topic models do.

\end{frame}

%%%%%%%

\begin{frame}[allowframebreaks]
\frametitle{References}
\printbibliography	
\end{frame}


\end{document}
