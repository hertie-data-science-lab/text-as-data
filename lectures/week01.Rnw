\documentclass{hertieteaching}
\usepackage{cancel}

\title{Quantitative Text Analysis}

\begin{document}

\maketitle

\begin{frame}{Text as data}

<<r, include = FALSE>>=
options(width = 60)
library(knitr)
library(tidyverse)
library(quanteda)
library(xtable)

opts_knit$set(error = FALSE)
opts_chunk$set(comment = NA,
               fig.align = "center",
               echo = FALSE)
options(xtable.floating = FALSE,
        latex.environments = "center",
        xtable.comment = FALSE,
        xtable.booktabs = TRUE)
@

This is not a course on linguistics.

\pause

How to learn about

\begin{itemize}
  \item party platforms
  \item legislative agendas
  \item parliamentary debates
  \item bloggers
  \item presidents
\end{itemize}

by counting (lots of) words?

\end{frame}
\begin{frame}{Transcendental question}


What are the \textit{conditions for the possibility}
 of learning about these
things by counting words?

In plainer language:
\begin{itemize}
  \item How could this possibly work?
\end{itemize}


\end{frame}
\begin{frame}{Big picture}

There is a \textit{message} or \textit{content} that cannot be directly observed,
e.g.
\begin{itemize}
  \item the topic of this lecture
  \item my position on some political issue
  \item the importance of defence issues to a some political party
\end{itemize}
and \textit{behaviour}, including \textit{linguistic behaviour}, e.g.
\begin{itemize}
  \item yelling
  \item writing
  \item lecturing
\end{itemize}
which \textit{can} be directly observed.

Although language can do things directly \parencite{Austin1962}, we'll focus on the \textit{expressed message} and the \textit{words}\ldots

\end{frame}
\begin{frame}{Communication}

\begin{columns}[T,onlytextwidth]
\column{0.5\textwidth}
To \textit{communicate} a message $\theta$ -- to inform, persuade, demand, or threaten -- a producer (the speaker or writer) \textit{generates} words of different kinds in
different quantities

\column{0.05\textwidth}
\column{0.45\textwidth}

\begin{tikzpicture}
\node(prepre) at (-.75,   0) [label={[yshift=-1ex]below:\textcolor{gray}{\ldots}}]{};
\node(pre) at (0,   0) [var,label=below:\textcolor{gray}{Tiber}]{};
\node(W1) at  (1, 0) [var,label=below:flow]{};
\node(W2) at  (2,   0)  [var,label=below:with]{};
\node(W3) at  (3, 0) [var,label=below:much]{};
\node(theta) at (2,1.5)  [lat,label=above:$\theta$]{};
\node(post) at (4, 0) [var,label=below:\textcolor{gray}{blood}]{};
\node(postpost) at (4.75,   0) [label={[yshift=-1ex]below:\textcolor{gray}{\ldots}}]{};
\draw  (theta) -- (pre);
\draw  (theta) -- (W1);
\draw  (theta) -- (W2);
\draw  (theta) -- (W3);
\draw  (theta) -- (post);
\end{tikzpicture}
\end{columns}

\vspace{2em}

\begin{columns}[T,onlytextwidth]
\column{0.5\textwidth}

To \textit{understand} a message the consumer (the hearer, reader, coder) uses those words to \textit{reconstruct} the message

\column{0.05\textwidth}
\column{0.45\textwidth}

\begin{tikzpicture}
\node(prepre) at (-.75,   0) [label={[yshift=-1ex]below:\textcolor{gray}{\ldots}}]{};
\node(pre) at (0,   0) [var,label=below:\textcolor{gray}{Tiber}]{};
\node(W1) at  (1, 0) [var,label=below:flow]{};
\node(W2) at  (2,   0)  [var,label=below:with]{};
\node(W3) at  (3, 0) [var,label=below:much]{};
\node(theta) at (2,1.5)  [lat,label=above:$\theta$]{};
\node(post) at (4, 0) [var,label=below:\textcolor{gray}{blood}]{};
\node(postpost) at (4.75,   0) [label={[yshift=-1ex]below:\textcolor{gray}{\ldots}}]{};
\draw  (pre) -- (theta);
\draw  (W1) -- (theta);
\draw  (W2) -- (theta);
\draw  (W3) -- (theta);
\draw  (post) -- (theta);
\end{tikzpicture}

\end{columns}

\end{frame}

\begin{frame}{Communication}

This process is
\begin{itemize}
  \item stable \parencite{Searle1995,Grice1993}
  \item conventional \parencite{Lewis2011}
  \item disruptible  \parencite{Riker.etal1996}
  \item empirically underdetermined \parencite{Quine1960,Davidson1985}
\end{itemize}

How to model this without having to solve the problems of linguistics (psychology, politics) first?

Rely on:
\begin{itemize}
  \item instrumentality
  \item reflexivity
  \item randomness
\end{itemize}

\end{frame}

\begin{frame}{Communication: Instrumentality}

Instrumentality from `them': Language use is a form of action \parencite{Wittgenstein1958,Austin1962,Krebs.Dawkins1984}

Note the distinction between
\begin{align*}
X &\text{~means~} Y & X &\text{~is used to mean~} Y
\end{align*}

Instrumentality from us:
\begin{itemize}
  \item we aren't actually interested in words themselves; that's are for linguists
  \item we aren't actually interested in what's in the head; that's for psychologists
\end{itemize}

Except as they help explain things we are interested in. Text is just data

\end{frame}

\begin{frame}{Communication: Reflexivity}

Politicians are often nice enough to talk as if they really do
communicate this way

~\\
\begin{quotation}
\noindent
My theme here has, as it were, four heads. [...] The first is
articulated by the word ``opportunity'' [...] the second is expressed
by the word ``choice'' [...] the third theme is summed up by the word
``strength'' [and] my fourth theme is expressed well by the word
``renewal''.
\end{quotation}

(Note however, these words occur 2, 7, 2, and 8 times in 4431 words)

\pause

~\\
\begin{quotation}
\noindent
A couple months ago we weren't expected to win this one, you know
that, right? We weren't...Of course if you listen to the pundits, we
weren't expected to win too much. And now we're winning, winning,
winning the country -- and soon the country is going to start winning,
winning, winning.
\end{quotation}

%\normalsize

\end{frame}
\begin{frame}{}

\begin{center}
\begin{figure}[hbt]
  \includegraphics[scale=0.6]{pictures/spock-detecting-large-quantities-sf.jpg"}
\end{figure}
\end{center}

\end{frame}


\begin{frame}{Communication and comparability}

Quantitative text analysis works best when language usage is
stable, conventionalized, and instrumental.

Implicitly, that means \textit{institutional language}, e.g.
\begin{itemize}
  \item courts
  \item legislatures
  \item op-eds
  \item financial reporting
\end{itemize}

Institution-specificity inevitably creates a
\textit{comparability} problem, e.g.
\begin{itemize}
  \item upper vs lower chamber vs parliamentary hearings
  \item bureaucracy vs lobby groups \parencite{Kluver2009}
  \item European languages \parencite{Proksch.etal2019}
\end{itemize}

\end{frame}

\begin{frame}{Instability}

We are going to design instruments to measure $\theta$ and are going to assume that the $\theta \longrightarrow W$ relationships are institutionally stable

What if they aren't?

\end{frame}

\begin{frame}{Rhetorical instability}

\begin{figure}[hbt]
  \includegraphics[scale=.105]{pictures/fightin2.png}
\end{figure}
\nocite{Monroe.etal2008}
\end{frame}

\begin{frame}{Algorithmic instability}

\begin{figure}[hbt]
  \includegraphics[scale=.9]{pictures/science-google-flu.jpg}
  \caption{\cite{Lazer.etal2014}}
\end{figure}
\end{frame}


\begin{frame}{Reflexive solutions}

Sometimes these actors are happy to solve comparability problems for us, e.g.
\begin{itemize}
  \item Lower court opinions \parencite{Corley.etal2011} or Amicus briefs \parencite{Collins.etal2015} \textit{embedded in} Supreme Court opinions
  \item ALEC model bills \textit{embedded in} state bills \parencite{Garrett.Jansa2015}
\end{itemize}

A perfect jobs for \textit{text-reuse} algorithms\ldots

\end{frame}

\begin{frame}{Communication: Randomness}

Why randomness?

You almost never \textit{say exactly the same words twice},
even when you haven't changed your mind about the message.

Hence words are the result of some kind of \textit{sampling process}.

We model this process as random because we don't know or care about
all the causes of variation (and because we're all secretly Bayesians)

Note: this is randomness \textit{conditional on the institution}

\end{frame}

\begin{frame}{Words as data}

What do we know about words as data?

They are \textit{difficult}
\begin{itemize}
  \item High dimensional
  \item Sparsely distributed (with skew)
  \item Not equally informative
\end{itemize}

\end{frame}

\begin{frame}{Difficult words}

<<results="hide">>=
load("data/corpus_uk_manif.rda")
main_parties <- c('Con', 'Lab', 'LD', 'UKIP', 'SNP')
election_years <- c(2017, 2015, 2010, 2005)
subcorp <- corpus_subset(corpus_uk_manif,
                         year %in% election_years &
                         party %in% main_parties)
dfmsubcorp <- dfm(subcorp, remove_punct = TRUE)

con2017dfm <- dfm_subset(dfmsubcorp, year == 2017 & party == "Con")
wcounts <- textstat_frequency(con2017dfm)

used <- nrow(wcounts) # used at least once
used_per <- 100 * used / nfeat(con2017dfm)
hapaxes <- nrow(wcounts[wcounts$frequency == 1,])
hapaxes_per <- 100 * hapaxes / nfeat(con2017dfm)
@

Example: Conservative party 2017 manifesto compared to other parties in two elections:

\begin{center}
\begin{tabular}{ll}\toprule
  High dimensional  & \Sexpr{used} word types (adult native english speakers know ~20-35,000) \\
   Sparse & Of these, the Conservatives only used \Sexpr{round(used_per, 1)}) \\
   Skewed &  Of these \Sexpr{round(hapaxes_per, 1)}\% words appear exactly once\\ \bottomrule
  \end{tabular}
\end{center}

\end{frame}

\begin{frame}{Difficult words}

More generally: the Zipf-Mandelbrot law \parencite{Zipf1932,Mandelbrot1966}
$$
F(W_i) \propto 1/{\text{rank}(W_i)^\alpha}
$$
where $\text{rank}(.)$ is the
frequency *rank* of a word in the vocabulary and
$\alpha\approx 1$

(This is a Pareto distribution in disguise)

\end{frame}


\begin{frame}{Difficult at all scales}

<<allscalesprep,results = "hide">>=
res <- bind_rows("Cons 1997" = wcounts,
                 "Corpus" = textstat_frequency(dfmsubcorp),
                 .id = "Source")
theme_set(theme_minimal())
@

\begin{columns}[T,onlytextwidth]
\column{0.45\textwidth}

<<allscalesorig,fig.align='center', fig.height = 3.2, fig.width = 3.2>>=
ggplot(sample_frac(res, 0.5),
       aes(rank, frequency, color = Source)) +
  geom_point(alpha = 0.5) +
  scale_color_manual(values = c("Corpus" = "grey",
                                "Cons 1997" = "black")) +
  theme(legend.position = c(.8, .8))
@

\column{0.1\textwidth}
\column{0.45\textwidth}

<<allscaleslog,fig.align='center', fig.height = 3.2, fig.width = 3.2>>=
ggplot(res, aes(rank, frequency, color = Source)) +
  geom_point() +
  scale_color_manual(values = c("Corpus" = "grey",
                                "Cons 1997" = "black"))+
    scale_x_log10() + scale_y_log10() +
  theme(legend.position=c(.8, .8))
@

\end{columns}

See also \cite{Chater.Brown1999} on scale invariance.

\end{frame}

\begin{frame}{Types and Tokens}

\begin{columns}[T,onlytextwidth]
\column{0.5\textwidth}

More generally: the Heaps-Herdan Law states that the number of
word types appearing for the first time after n tokens is
$$
D(n) = K n^\beta
$$
where $K$ is between 10 and 100 and $\beta \approx 0.5$ for English.

~\\

Here's the Conservative party manifesto

\column{0.05\textwidth}
\column{0.45\textwidth}

<<heaps,warnings=FALSE,fig.align='center', fig.height = 3.2, fig.width = 3.2>>=
tks <- as.character(tokens_tolower(tokens(subcorp[["UK_natl_2017_en_Con.txt"]], remove_punct = TRUE)))
unique_tks <- unique(tks)
ll <- list()
newtypes <- integer(length(unique_tks))
for (i in seq_along(newtypes)) {
  if (is.null(ll[[ tks[i] ]]))
    ll[[ tks[i] ]] <- 1
  newtypes[i] <- length(ll)
}
dd <- data.frame(newtype = newtypes, token = 1:length(newtypes))

tksall <- sample(as.character(tokens_tolower(tokens(subcorp, remove_punct = TRUE))))
unique_tksall <- unique(tksall)
ll <- list()
newtypes <- integer(length(unique_tksall))
for (i in seq_along(newtypes)) {
  if (is.null(ll[[ tksall[i] ]]))
    ll[[ tksall[i] ]] <- 1
  newtypes[i] <- length(ll)
}
ddall <- data.frame(newtype = newtypes, token = 1:length(newtypes))

ggplot(ddall, aes(token, newtype)) +
  geom_abline(intercept = 0, slope = 1, col = "gray", linetype = "dashed") +
  geom_line() +
  scale_x_log10() + scale_y_log10() +
  labs(x = "tokens", y = "word types")
@

\end{columns}

\end{frame}


\begin{frame}{Frequency and interestingness}

Frequency is inversely proportional to substantive interestingness

~\\

\begin{columns}[T,onlytextwidth]
\column{0.33\textwidth}

\begin{center}
<<inter,results="asis">>=
tbl <- textstat_frequency(dfmsubcorp)[1:6, 1:2]
names(tbl) <- c("Word", "Freq.")
print(xtable(tbl, digits = 0))
@

~\\
Top 10
\end{center}
\column{0.33\textwidth}


\begin{center}

<<bottomten,results="asis">>=
tbl <- tail(textstat_frequency(dfmsubcorp), 6)[, 1:2]
names(tbl) <- c("Word", "Freq.")
print(xtable(tbl, digits = 0))
@

~\\
Bottom ten
\end{center}

\column{0.33\textwidth}
\begin{center}
<<topfiltered,results="asis">>=
tf <- filter(textstat_frequency(dfmsubcorp),
             !(feature %in% stopwords("english")))
tbl <- head(tf, 6)[, 1:2]
names(tbl) <- c("Word", "Freq.")
print(xtable(tbl, digits = 0))
@

~\\
Top ten minus \textit{stopwords}
\end{center}

\end{columns}

\end{frame}

\begin{frame}{Dealing with difficult words}

Removing stopwords, while standard in computer science, is not necessarily better\ldots

Example:
\begin{itemize}
\item Standard collections contain, `him', `his', `her' and `she'.
\item Words you'd want to keep when analyzing an abortion debates.
\end{itemize}

\end{frame}

\begin{frame}{Dealing with difficult words}

For large amounts of text summaries are not enough.

We need a \textit{model} to provide assumptions about
\begin{itemize}
  \item \textit{equivalence}
  \item \textit{exchangeability}
\end{itemize}


%Two words may
%\begin{tabular}{ll}
%\textsl{refer} to the same thing & (co-reference) \\
%\textsl{mean} the same thing & (synonymy) \\
%are \textsl{adjectives} & (syntactic role)\\
%\textsl{express} hostility, identity, \ldots & (pragmatics)\\
%\end{tabular}

Text as data started off making most use of equivalence, and ended up with increasingly sophisticated versions of exchangeability

Since ontogeny recapitulates phylogeny, let's walk through some standard text processing steps, asserting equivalences along the way\ldots

\end{frame}
\begin{frame}[t]\frametitle{Punctuation invariance}

\begin{quote}As I look ahead I am filled with foreboding.  Like the Roman I seem to see `the river Tiber flowing with much blood'\ldots ''\\
(E. Powell, 1968)
\end{quote}

\pause

\begin{center}
{\small
\begin{tabular}{ll}\toprule
index & token\\ \midrule
1 & as\\
2 & i\\
3 & look\\
4 & ahead\\
5 & i\\
6 & am\\
7 & \ldots\\ \bottomrule
\end{tabular}
~~~~~~~~~~
\begin{tabular}{ll}\toprule
index & token\\ \midrule
1 & like\\
2 & the\\
3 & roman\\
4 & i\\
5 & seem\\
6 & to\\
7 & \ldots\\ \bottomrule
\end{tabular}
}
\end{center}


\end{frame}
\begin{frame}[t]\frametitle{Lexical univocality}

\begin{center}
{\small
\begin{tabular}{ll}\toprule
type & count\\ \midrule
as & 1\\
i & 2\\
look & 1\\
ahead & 1\\
am & 1\\
\ldots & \ldots\\ \bottomrule
\end{tabular}
~~~~~~~~~~
\begin{tabular}{ll}\toprule
token & count\\ \midrule
like & 1\\
the & 1\\
roman & 1\\
i & 1\\
seem & 1\\
to & 1\\
\ldots & \ldots\\ \bottomrule
\end{tabular}
}
\end{center}

\end{frame}
\begin{frame}[t]\frametitle{Order invariance}

\begin{center}
{\small
\begin{tabular}{rlll}\toprule
&         & unit    & \\ \midrule
&         & `doc' 1 & `doc' 2 \\ \midrule
type      & ahead   & 1    & 0 \\
& am      & 1    & 0 \\
& as      & 1    & 0 \\
& i       & 2    & 1 \\
& like    & 0    & 1\\
& look    & 1    & 0 \\
& roman   & 0    & 1 \\
& seem    & 0    & 1 \\
& the     & 0    & 1 \\
& to      & 0    & 1\\
& \ldots  & \ldots & \ldots \\ \bottomrule
\end{tabular}
}
\end{center}

\end{frame}

\begin{frame}[t]\frametitle{Count data}

We have turned a corpus into a \textit{contingency table}.
\begin{itemize}
\item (Or a term-document / document-term / document-feature matrix, in the lingo)
\end{itemize}

\pause

~\\
Everything you learned in your categorical data analysis course applies
\begin{itemize}
\item except that the variables of interest: $\theta$ are \textit{not observed}
\end{itemize}

%%%%%%%%%%%

\end{frame}
\begin{frame}[t]\frametitle{What we want}

\begin{center}
{\small
\begin{tabular}{rllllllllll}\toprule
      & ahead & am & i & like & look & & \\ \midrule
doc 1 & 1     & 1  & 2 & 0    & 1    & \ldots & \textcolor{gray}{$\theta_{doc1}$} \\
doc 2 & 0     & 0  & 1 & 1    & 0    & \ldots & \textcolor{gray}{$\theta_\text{doc2}$} \\ \midrule
      & \textcolor{gray}{$\beta_\text{ahead}$}
      & \textcolor{gray}{$\beta_\text{am}$}
      & \textcolor{gray}{$\beta_\text{i}$}
      & \textcolor{gray}{$\beta_\text{like}$}
      & \textcolor{gray}{$\beta_\text{look}$} \\ \bottomrule
\end{tabular}
}
\end{center}
~\\\

For each research problem involving content analysis we need to ask:
\begin{itemize}
\item What counts as a \textit{word}?
\item What counts as a \textit{document}?
\item What \textit{structure} does $\theta$ has
\item What is \textit{observed}, what is \textit{assumed}, and what is \textit{inferred}?
\item What the \textit{relationship} is between $\theta$ and the words? The model
\end{itemize}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[allowframebreaks]
\frametitle{References}

\printbibliography

\end{frame}

\end{document}





%
%## Labs
%
%```{r, out.width = "80%", fig.cap = "'You must press the button. I'm a scientist you know!'"}
%knitr::include_graphics("pictures/milgram-machine.jpg")
%```
%
%## Menu
%
%**Session 1: How could this possibly work?**
%
%Session 2: Dictionary-based 'classical' content analysis and topic models
%
%Session 3: Scaling models
%
%Session 4: Getting hold of the data
%
%## Focus
%
%Assumptions
%
%Interpretation
%
%Justification
%
%Pitfalls
%
%Mechanics (in Lab)
%
%##
%
%```{r, out.width = '80%'}
%knitr::include_graphics("pictures/etinarcadiaego.jpg")
%```
%
%


%
%```{r, out.width = '90%'}
%knitr::include_graphics("pictures/grey-just-rile.pdf")
%```
%
%## Statistical assumptions about words
%
%Word counts/rates are conditionally Poisson:
%$$
%W_j ~\sim~ \text{Poisson}(\textcolor{red}{\lambda_{j}})
%$$
%
%Expected $W_{j}$ (and its variance) is $\lambda_{j}$
%
%Models are naturally *multiplicative*. Rates increase by
%10%, decrease by 20%
%
%. . .
%
%The rate $\lambda$ depends on $\theta$ in a more or less complex fashion.
%
%## Statistical models of words: Poisson
%
%```{r, out.width="80%"}
%knitr::include_graphics("pictures/geiger-counter.jpg")
%```
%
%## Statistical models of words: Multinomial
%
%For *fixed* document lengths, counts are conditionally Multinomial:
%$$
%W_{1}\ldots W_{V} ~\sim~ \text{Multinomial}(W_{1}\ldots W_{V} \mid \textcolor{red}{\pi_{1}}\ldots\textcolor{red}{\pi_{V}}, N_i)
%$$
%
%Expected $W_{i}$ is $N\pi_{i}$
%
%Covariance of $W_{i}$ and $W_{j}$ is $-N \pi_{i}\pi_{j}$
%
%Negative covariance is due to the 'budget constraint'
%
%## Statistical models of words: Multinomial
%
%```{r, out.width = '80%'}
%knitr::include_graphics("pictures/20-sided-die.jpg")
%```
%## Implication: Absence is an observation
%
%Note: Don't be fooled...
%
% - Statistical models of text deal with *absence* as well as presence:
%   zeros count
% - Absence is informative *to the extent it is surprising*
% - Surprise implies expectations; expectations imply a model.
%
%
%## Exchangeability and the 'bag of words'
%
%```{r, out.width = '90%'}
%knitr::include_graphics("pictures/fdp-en.png")
%```
%
%## Intuition: the marginal challenge
%
%The original:
%
%> Reasonable relief of the working middle class and
%> debt reduction do not exclude each other but are
%> complementary. That proved to be true during
%> the past four years. We oppose higher taxes on
%> citizens and businesses. They prevent growth and
%> kill jobs, thus putting at risk the very existence of
%> countless workers and their families.
%
%. . .
%
%Given the *marginal distribution of word types* here a.k.a. the 'bag of words' and
%a long afternoon...
%
%- Just how different could the meaning of the resulting document be?
%
%
%## Slightly different...
%
%A possible reconstruction
%
%> We oppose higher taxes because they prevent growth and
%> kill jobs, risking the very existence of
%> countless families and businesses. Debt reduction is
%> complementary to reasonable relief of the
%> working middle class; they do not exclude each other.
%> That proved to be true in the last four years.
%
%But much the same sense
%
%## Looser constraints
%
%We can make this a more semantic challenge by only demanding
%the substantically interesting word margins are maintained.
%
%> businesses reasonable existence countless
%> reduction families prevent risking working exclude oppose
%> higher growth relief middle proved taxes class years kill
%> jobs debt true last four complementary
%
%(Removing stopwords mostly just removes *grammatical* constraints)
%
%## But negation!
%
%Wait, if we can add grammatical functors at will, could we
%make the opposite meaning by negating everything?
%
%. . .
%
%In principle (and in practice for some discursive forms) yes, yes we could.
%And the bag of words assumption would fail
%
%. . .
%
%An interesting *empirical fact* about political discourse is that
%actors do not tend to disagree by negation but by redirection or
%diversion.
%
%- Simple version: You talk about the environment, I talk about economic growth
%- Sophisticated version: The 'heresthetic' [@Riker.etal1996]
%
%## What is this content $\theta$?
%
%What is the content in content analysis?
%
% - Documents are *mixtures of categories*: policy agenda of a speech
% - Documents have *categories*: topic of a press release
% - Documents have *positions*: ideological position of a legal brief
%
%Important things we will not discuss:
%
% - Documents as collections of facts and assertions
% - Documents as networks
%
%## Specifically: NLP
%
%'Natural Language Processing' (NLP) tasks, some of which
%may be of use to us:
%
%- Segmentation / Tokenization: Finding where the words and sentences are
%- Lemmatization: Stripping a word down to its 'root'
%- Part of Speech (POS) tagging: Associating grammatical
%  roles with words (noun, verb, determiner, preposition, etc.)
%- Parsing: Associating grammatical structures with sentences
%  (typically dependency graphs, but sometimes trees and
%  feature structures)
%
%
%## Specifically: NLP
%
%- Named Entity Recognition (NER): Identifying people, places, and things
%- Information Extraction (IE): Extracting 'facts' (who did
%  what to whom, when)
%
%. . .
%
%Tools:
%
%- [Spacy](https://spacy.io) (Python) via `{spacyr}`,
%  [Stanford NLP tools](https://nlp.stanford.edu/software/),
%  (Java) via `{CoreNLP}` (or `{cleanNLP}` which wraps both).
%  Also [OpenNLP](http://opennlp.apache.org/) (Java) via
%  `{openNLP}`.
%
%## What is this content $\theta$?
%
%```{r, out.width="80%"}
%knitr::include_graphics("pictures/tad-picture.png")
%```
%
%## What is this content $\theta$?
%
%```{r, out.width="80%"}
%knitr::include_graphics("pictures/measurement-validity.png")
%```
%
%## Commitment anxiety
%
%What more are we committing to in this quantitative content analysis
%framework?
%
%- substantive theory $\neq$ textual measurement
%
%but they do have implications for one another
%
%## A Theory/measurement distinction
%
%Discourse analysis tends to *tightly couple* theory and measurement
%
%\footnotesize
%**A complete package**
%
%Although discourse analysis can be applied to all areas of
%research, it cannot be used with all kinds of theoretical
%framework. Crucially, it is not to be used as a method of
%analysis detached from its theoretical and methodological
%foundations. Each approach to discourse analysis that
%we present is not just a method for data analysis, but a
%theoretical and methodological whole - a complete package.
%[...] In discourse and analysis theory and method are
%intertwined and researchers must accept the basic
%philosophical premises in order to use discourse analysis as
%their method of empirical study.
%\normalsize
%
%[@Jorgensen.Phillips2002]
%
%## Not that
%
%. . .
%We will try as far as possible to separate theory and
%measurement (and concentrate on the second).
%
% - Our concerns: validity, stability
% - Rely on: transparency, reliability, replicability
%
%
%## The 'digital trace'
%
%\footnotesize
%
%"[J]ust as the invention of the telescope revolutionized the study of the heavens, so too by rendering the unmeasurable measurable, the technological revolution in mobile, Web, and Internet communications has the potential to revolutionize our understanding of ourselves and how we interact . [T]hree hundred years after Alexander Pope argued that the proper study of mankind should lie not in the heavens but in ourselves, we have finally found our telescope. Let the revolution begin."
%
%Duncan Watts (2011)
%\normalsize
%
%. . .
%
%The stars do not look differently when people are watching them.
%But our Instagram accounts do.
%
%## The 'digital trace'
%
%(or less romantically 'digital exhaust')
%
%In the worst case, our texts are
%
%- non-representative (all this guy does is eat)
%- incomplete (never sleeps)
%- positivity biased (but often pets puppies)
%- algorithmically confounded (and I see him everywhere)
%- ethically challenging (so I downloaded all his posts)
%
%. . .
%
%Just like most other kinds of data about people in the
%social sciences...
%
%
%
%## Lab time
%
%```{r, out.width = '90%'}
%knitr::include_graphics("pictures/hieroglyphic-keyboard-cm.jpg")
%```
%## References  {.allowframebreaks}
%
%\tiny

